# E2E Testing Configuration for LiveTemplate CI/CD Pipeline
# This file configures the behavior of E2E tests in GitHub Actions

# Test execution settings
execution:
  # Maximum time for each test group
  timeout_minutes: 30
  
  # Number of retry attempts for flaky tests
  retry_attempts: 3
  
  # Delay between retry attempts (seconds)
  retry_delay: 30
  
  # Enable parallel test execution
  parallel_execution: true
  
  # Maximum parallel jobs
  max_parallel_jobs: 6

# Browser configuration
browsers:
  # Default browser for testing
  default: chrome
  
  # Browser-specific settings
  chrome:
    # Chrome binary paths for different platforms
    linux_path: google-chrome-stable
    macos_path: "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
    windows_path: "C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe"
    
    # Chrome flags for CI environment
    flags:
      - --no-sandbox
      - --disable-gpu  
      - --headless
      - --disable-dev-shm-usage
      - --disable-background-timer-throttling
      - --disable-backgrounding-occluded-windows
      - --disable-renderer-backgrounding
      - --disable-web-security
      - --disable-features=VizDisplayCompositor
      
    # Window size for screenshots
    window_size:
      width: 1920
      height: 1080

  chromium:
    linux_path: chromium-browser
    flags:
      - --no-sandbox
      - --disable-gpu
      - --headless

# Screenshot configuration
screenshots:
  # Enable screenshots on test failures
  enabled: true
  
  # Enable screenshots for successful tests
  success_screenshots: false
  
  # Screenshot quality (0-100)
  quality: 90
  
  # Screenshot format
  format: png
  
  # Maximum screenshots per test
  max_per_test: 10

# Artifact configuration
artifacts:
  # Enable artifact collection
  enabled: true
  
  # Artifact retention in days
  retention_days: 30
  
  # Maximum artifact size per job (MB)
  max_size_mb: 100
  
  # Types of artifacts to collect
  types:
    - test_results
    - performance_metrics
    - screenshots
    - failure_logs
    - coverage_reports
    - browser_logs

# Performance monitoring
performance:
  # Enable performance tracking
  enabled: true
  
  # Performance thresholds
  thresholds:
    # Maximum average page load time (ms)
    max_page_load_ms: 5000
    
    # Maximum fragment generation time (ms)
    max_fragment_generation_ms: 500
    
    # Maximum memory usage per test (MB)
    max_memory_usage_mb: 500
    
    # Minimum cache hit ratio (%)
    min_cache_hit_ratio: 60
  
  # Metrics to collect
  metrics:
    - page_load_time
    - fragment_generation_time
    - memory_usage
    - cache_hit_ratio
    - browser_action_duration
    - network_requests
    - javascript_errors

# Test groups configuration
test_groups:
  infrastructure:
    description: "Basic infrastructure and browser setup tests"
    timeout_minutes: 10
    retry_attempts: 3
    required: true
    
  browser-lifecycle:
    description: "Full browser lifecycle and DOM manipulation tests"
    timeout_minutes: 15
    retry_attempts: 3
    required: true
    
  performance:
    description: "Performance benchmarks and timing tests"
    timeout_minutes: 20
    retry_attempts: 2
    required: false
    
  error-scenarios:
    description: "Error handling and edge case tests"
    timeout_minutes: 15
    retry_attempts: 3
    required: false
    
  concurrent-users:
    description: "Concurrent user simulation and load tests"
    timeout_minutes: 25
    retry_attempts: 2
    required: false
    
  cross-browser:
    description: "Cross-browser compatibility tests"
    timeout_minutes: 20
    retry_attempts: 3
    required: false

# Flakiness detection
flakiness:
  # Enable flakiness detection
  enabled: true
  
  # Minimum retry count to consider a test flaky
  min_retry_threshold: 2
  
  # Maximum allowed flaky tests before failing the pipeline
  max_flaky_tests: 3
  
  # Auto-retry flaky tests
  auto_retry: true
  
  # Report flaky tests as warnings instead of failures
  report_as_warnings: true

# Reporting configuration
reporting:
  # Enable detailed HTML reports
  html_reports: true
  
  # Enable JUnit XML reports
  junit_reports: true
  
  # Enable JSON reports for API consumption
  json_reports: true
  
  # Include performance metrics in reports
  include_performance: true
  
  # Include screenshots in HTML reports
  include_screenshots: true
  
  # Generate trend reports
  trend_analysis: true

# Docker configuration (for containerized testing)
docker:
  # Enable Docker-based testing
  enabled: false
  
  # Chrome image for Docker testing
  chrome_image: selenium/standalone-chrome:latest
  
  # Chrome image version
  chrome_version: latest
  
  # Docker options
  options:
    - --shm-size=2g
    - --memory=2g
    - --cpus=2

# Environment-specific settings
environments:
  ci:
    # CI-specific overrides
    screenshots:
      enabled: true
      success_screenshots: false
    performance:
      thresholds:
        max_page_load_ms: 10000  # More lenient in CI
    execution:
      retry_attempts: 3
      
  local:
    # Local development overrides
    screenshots:
      enabled: true
      success_screenshots: true
    execution:
      retry_attempts: 1
      parallel_execution: false
    browsers:
      chrome:
        flags: []  # No special flags for local development
        
  pr:
    # Pull request specific settings
    test_groups:
      performance:
        required: false
      concurrent-users:
        required: false
    execution:
      timeout_minutes: 20  # Shorter timeout for PRs

# Integration settings
integrations:
  # Slack notifications
  slack:
    enabled: false
    webhook_url: ""
    notify_on_failure: true
    notify_on_success: false
    
  # Email notifications  
  email:
    enabled: false
    recipients: []
    notify_on_failure: true
    
  # GitHub status checks
  github:
    enabled: true
    required_checks:
      - infrastructure
      - browser-lifecycle
    optional_checks:
      - performance
      - error-scenarios
      - concurrent-users
      - cross-browser